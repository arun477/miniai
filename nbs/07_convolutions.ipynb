{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_export conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c837d9a",
   "metadata": {},
   "source": [
    "### Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import default_collate\n",
    "from miniai.training import *\n",
    "from miniai.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip,pickle,matplotlib.pyplot as plt,numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800b238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('../data/mnist.pkl.gz', 'rb') as f: data = pickle.load(f, encoding='latin')\n",
    "((x_train,y_train),(x_val,y_val),_) = data\n",
    "(x_train,y_train,x_val,y_val) = map(torch.tensor,(x_train,y_train,x_val,y_val))\n",
    "(x_train.shape,y_train.shape,x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c569cb",
   "metadata": {},
   "source": [
    "### Understanding Convolution Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe6113",
   "metadata": {},
   "source": [
    "* https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c (How it differ from norm FNN)\n",
    "* https://arxiv.org/pdf/1603.07285.pdf (Convolution Arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641e28f",
   "metadata": {},
   "source": [
    "\n",
    "* The mechanics is quite simple, you take your input image and flat it out all the pixel as a single vector and start multiply each pixel with it's corresponding weights and bias then you will get one output, and if you want multiple output, you can copy this same process, just with different weights for the each output, and finaly pass all the outputs through some non-linear function. this is our simple FNN operation. the only difference is, we are going to keep the same weights across multiple outputs and the number of weights are not going to be same as number of input pixels, actually number of weights will be much less than number of input pixels. because of this, each neurons will only be multiplied with few input pixels. But we don't want to miss any input pixels, to solve this issue what we can do is, each output focus on different set of pixels, means for each output, wieghts (same weights) will be multiplied with different set of input pixels, so this way, we cover all the input pixels.\n",
    " <br/>\n",
    "    \n",
    "* Now we need to choose which set of weights each outputs should focus on. this is done, simply by taking weights and reshaping it into 2D format, example if number of weights are 9 then it will be reshaped as 3x3. then you place it across image horizontal and vertical directions, each placement is the set of weights for each output.  that's it. this is \"Convolution\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b42e56",
   "metadata": {},
   "source": [
    "##### Convolution Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddbc9c",
   "metadata": {},
   "source": [
    "* In Convolution operation output number of neurons depends on input size, number of weights, padding, and stride.\n",
    " <br/>\n",
    " \n",
    "* Input size: number of input pixels.\n",
    " <br/>\n",
    " \n",
    "* Padding: extra number of dummy pixels added to the inputs.\n",
    "<br/>\n",
    "\n",
    "* Stride: number of pixels needs to be skipped during sliding.\n",
    "<br/>\n",
    "\n",
    "* Number of weights (kernel size): size of the shared weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55bbbb",
   "metadata": {},
   "source": [
    "##### Deriving General Formula for the Output Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ea86f",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Asumme input is 1D, as it can be extended to 2D.\n",
    "    # Input Size: W\n",
    "    # Kernel Size: K\n",
    "    # Number of Strides: S\n",
    "    # Number of Paddings: P\n",
    "    # Output Size: O\n",
    "    \n",
    "    # Case 1:\n",
    "        # S=1, 0<K<W, P=0\n",
    "        # In this case kernel sliding one pixel at a time on the input pixels\n",
    "        # until it reaches the end where kernel no longer can be contained inside the pixels, so we exclude \n",
    "        # thoes end pixels.\n",
    "        \n",
    "        O = (W-K+1)\n",
    "        \n",
    "    # Case 2:\n",
    "        # P>0, S=1, 0<K<W\n",
    "        # If padding is greater than 0, which means those number extra dumpy pixels both the size,\n",
    "        # so we need to multiply with 2.\n",
    "        \n",
    "        O = (W-k+1+2*P)\n",
    "        \n",
    "    # Case #:\n",
    "        # P>0, S>1, 0<K<W\n",
    "        # When stride is greater than 0, we are going to skip those many pixels.\n",
    "        \n",
    "        O = ((W-K+2*P)/S) + 1\n",
    "        \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddc14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-python_main-py",
   "language": "python",
   "name": "conda-env-python_main-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
