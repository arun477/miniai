{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbc9ee5",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2237b0",
   "metadata": {},
   "source": [
    "<p>\n",
    "<a href='http://www.paulgraham.com/articles.html' target='_blank'>Paul Graham essays</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87c37379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import bs4\n",
    "# import json\n",
    "\n",
    "# link = 'http://www.aaronsw.com/2002/feeds/pgessays.rss'\n",
    "# res = requests.get(link)\n",
    "# sp = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "# es_links = [[ele for ele in blck.get_text().split('\\n') if ele][0]  for blck in sp.find_all('item') ]\n",
    "# print('total essays:', len(es_links))\n",
    "\n",
    "# def get_txt(link):\n",
    "#     try:\n",
    "#         res = requests.get(link)\n",
    "#         sp = bs4.BeautifulSoup(res.text)\n",
    "#         for br in sp('br'):\n",
    "#             br.replace_with('\\n')\n",
    "#         return sp.find('body').find('table').find('font').get_text()\n",
    "#     except:\n",
    "#         print(f'failed: {link}')\n",
    "# es_txts = [get_txt(link) for link in es_links]\n",
    "\n",
    "# with open('./paulgraham_essays.json', 'w') as dest:\n",
    "#     dest.write(json.dumps([ele for ele in es_txts if ele]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf65455",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01b4096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d0180",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "30838444",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./paulgraham_essays.json', 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f01aaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = \"\"\n",
    "for ele in data:\n",
    "    txts += ele + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e5279b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 1845065, unique chars: 103\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(txts))\n",
    "char_sz,data_sz = len(chars), len(txts)\n",
    "print(f'total chars: {data_sz}, unique chars: {char_sz}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "538fd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_to_idx = {ch:idx for idx,ch in enumerate(chars)}\n",
    "idx_to_ch = {idx:ch for ch,idx in ch_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2ba48389",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sz = 100\n",
    "sq_sz = 25\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0da9c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxh = np.random.randn(h_sz,char_sz)*0.01\n",
    "whh = np.random.randn(h_sz,h_sz)*0.01\n",
    "why = np.random.randn(char_sz,h_sz)*0.01\n",
    "bh = np.zeros((h_sz, 1))\n",
    "by = np.zeros((char_sz, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bb6364f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 103), (100, 100), (103, 100))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wxh.shape,whh.shape,why.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9627ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p=0,0\n",
    "mwxh,mwhh,mwhy = np.zeros_like(wxh),np.zeros_like(whh),np.zeros_like(why)\n",
    "mbh,mby = np.zeros_like(bh),np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/char_sz)*sq_sz # initial cross-entropy loss set based on uniform probs of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "31be558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = np.zeros((h_sz, 1))\n",
    "inputs = [ch_to_idx[ch] for ch in txts[p:p+sq_sz]]\n",
    "targets = [ch_to_idx[ch] for ch in txts[p+1:p+sq_sz+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "454f876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, hs, ys, ps = {}, {}, {}, {}\n",
    "hs[-1] = np.copy(hprev)\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "92578320",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((char_sz, 1))\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t-1])+ bh)\n",
    "    ys[t] = np.dot(why, hs[t]) + by\n",
    "    ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "    loss += -np.log(ps[t][targets[t], 0])e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6cb46c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.88311805955101"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1555ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dwxh,dwhh,dwhy = np.zeros_like(wxh),np.zeros_like(whh),np.zeros_like(why)\n",
    "dbh,dby = np.zeros_like(bh),np.zeros_like(by)\n",
    "dhnext = np.zeros_like(hs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "37240955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1\n",
    "    dwhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(why.T, dy) + dhnext\n",
    "    dhraw = (1-hs[t]*hs[t])*dh\n",
    "    dbh += dhraw\n",
    "    dwxh += np.dot(dhraw, xs[t].T)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72723062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_main] *",
   "language": "python",
   "name": "conda-env-python_main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
