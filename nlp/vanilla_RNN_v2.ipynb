{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbc9ee5",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2237b0",
   "metadata": {},
   "source": [
    "<p>\n",
    "<a href='http://www.paulgraham.com/articles.html' target='_blank'>Paul Graham essays</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bc6e6",
   "metadata": {},
   "source": [
    "```python \n",
    "    import requests\n",
    "    import bs4\n",
    "    import json\n",
    "\n",
    "    link = 'http://www.aaronsw.com/2002/feeds/pgessays.rss'\n",
    "    res = requests.get(link)\n",
    "    sp = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    es_links = [[ele for ele in blck.get_text().split('\\n') if ele][0]  for blck in sp.find_all('item') ]\n",
    "    print('total essays:', len(es_links))\n",
    "\n",
    "    def get_txt(link):\n",
    "        try:\n",
    "            res = requests.get(link)\n",
    "            sp = bs4.BeautifulSoup(res.text)\n",
    "            for br in sp('br'):\n",
    "                br.replace_with('\\n')\n",
    "            return sp.find('body').find('table').find('font').get_text()\n",
    "        except:\n",
    "            print(f'failed: {link}')\n",
    "    es_txts = [get_txt(link) for link in es_links]\n",
    "\n",
    "    with open('./paulgraham_essays.json', 'w') as dest:\n",
    "        dest.write(json.dumps([ele for ele in es_txts if ele]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91009e7d",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eabdafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3d19d",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ab50e",
   "metadata": {},
   "source": [
    "<p>\n",
    " Let's take paulgraham essays and try to create language model using simple vanilla RNN.<br/>\n",
    " To keep it simple we use character level model to avoid big vocab size issue.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c6d745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./paulgraham_essays.json', 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abacf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = \"\"\n",
    "for ele in data:\n",
    "    txts += ele + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b0f2826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:103, data char size: 1845065\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(txts))\n",
    "vocab_sz, n_char = len(vocab),len(txts)\n",
    "print(f'vocab size:{vocab_sz}, data char size: {n_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76768645",
   "metadata": {},
   "source": [
    "<p> Data char size is too big, let's try first small number char size by truncating </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25eb50aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:69, data char size: 10000\n"
     ]
    }
   ],
   "source": [
    "max_sz = 10000\n",
    "txts = txts[:max_sz]\n",
    "vocab = list(set(txts))\n",
    "vocab_sz, n_char = len(vocab),len(txts)\n",
    "print(f'vocab size:{vocab_sz}, data char size: {n_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339273f6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28191c",
   "metadata": {},
   "source": [
    "<p style=\"line-height:2\">\n",
    "   * Our model is a simple RNN (Recurrent Nueural Netwrok) model. <br/>\n",
    "   * RNN is speicific way of arranging neural network layers so that it can model sequence data like texts. <br/>\n",
    "   * Usual feed-forward neural network (FNN) has problem with handling sequence data like texts, the problem is keeping track of dependencies in the sequence so that it can produce next element in the sequence, in the case of texts it may be next word/char.<br/>\n",
    "   * problem-1: in FNN sequence order got destroyed, but order is important in sequence data like texts. <br/>\n",
    "   * problem-2: FNN takes entire sequence in a single go but we need to input the sequence one element/char at a time and get the next predicted element/char in the sequence, so that we can train our model using the predicted element/char against actual next element/char in the sequence. if we want to do this in FNN then it will require variying input size but FNN requires pre-defined input and output size.<br/>\n",
    "   * So basically RNN is just a modified version of FNN that can handle above mentioned problems and also able to train params using backpropagation.<br/>\n",
    "   \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f9699",
   "metadata": {},
   "source": [
    "#### Input and Label Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55305566",
   "metadata": {},
   "source": [
    "<p style=\"line-height:2\">\n",
    "    * We can't input entire sequence data into network, we need to split the sequence into multiple small chunks of sequence so that our system can handle one at a time.<br/>\n",
    "    * Our goal for the model is it should take one element at a time and produce next element, so label will be the next element given the previous element. <br/>\n",
    "    * Also we need to convert our each char into number, because we can't process raw text char, simple thing to do is assign unique int to each char in our vocab.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0e5d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {ch:i for i,ch in enumerate(vocab)}\n",
    "idx_to_char = {i:ch for ch,i in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b72e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_sz = 25\n",
    "inputs = txts[0:seq_sz]\n",
    "targets = txts[1:seq_sz+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff323d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'January 2023\\n\\n(Someone fe'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0674e43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anuary 2023\\n\\n(Someone fed'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2df27393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(txts, n_char, seq_sz):\n",
    "    for i in range(n_char):\n",
    "        if (i+seq_sz+1)>n_char:\n",
    "            return ([], [])\n",
    "        yield ([char_to_idx[ch] for ch in txts[i:i+seq_sz]], [char_to_idx[ch] for ch in txts[i+1:i+1+seq_sz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8bc16aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(txts, n_char, seq_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "813010c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,targets = next(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3bcefe52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 60, 29, 11]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d406579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 29, 11, 60]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67071933",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb973476",
   "metadata": {},
   "source": [
    "<p style=\"line-height:2\">\n",
    "     * wxh : weights of the first hidden unit, this is basically normal FNN, this will take one element at a time. <br/>\n",
    "     * whh : weights of the second hidden unit, which takes the output of the previous elements hidden layer output. <br/>\n",
    "     * why : weights of the third hidden unit, which takes sum of the first two hidden units output as input and output the logits for the next chars. <br/>\n",
    "                                                                                                 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b470a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sz = 100\n",
    "input_sz = vocab_sz # one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "15329aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxh = np.random.randn(hidden_sz,input_sz)*0.01\n",
    "whh = np.random.randn(hidden_sz, hidden_sz)*0.01\n",
    "why = np.random.randn(vocab_sz, hidden_sz)*0.01\n",
    "bh = np.zeros((hidden_sz, 1))\n",
    "by = np.zeros((vocab_sz, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab2e8e",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7498106d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(inputs,targets):\n",
    "    allhs,alllogits,allps,alleles = {},{},{},{}\n",
    "    allhs[-1] = np.zeros((hidden_sz, 1))\n",
    "    loss = 0\n",
    "    for t in range(len(inputs)):\n",
    "        ele_t = np.zeros((vocab_sz, 1)) # one-hot representation\n",
    "        alleles[t] = ele_t\n",
    "        ele_t[inputs[t]] = 1 \n",
    "        hs = np.tanh(np.dot(wxh, ele_t) + np.dot(whh, allhs[t-1]) + bh) # hidden state\n",
    "        allhs[t] = hs\n",
    "        logits = np.dot(why, hs) + by # raw score for each char\n",
    "        alllogits[t] = logits\n",
    "        ps = np.exp(logits)/np.sum(np.exp(logits)) # get probs for each char\n",
    "        allps[t] = ps\n",
    "        loss += -np.log(ps[targets[t]][0])\n",
    "    return loss,allhs,alllogits,allps,alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "30e3b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,*_ = forward(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fece0adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.85149713440114"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a8163",
   "metadata": {},
   "source": [
    "If we assume initialy that probability of picking correct char is from uniform distribution, we can estimate what would be the inital loss would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3b65689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.85266261493149"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1/vocab_sz)*seq_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad702eb5",
   "metadata": {},
   "source": [
    "#### Calculate Gradient of the Loss Function based on Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42e8fe",
   "metadata": {},
   "source": [
    "<p style=\"line-height:2\">\n",
    "    * Gradient is basically rate of change of the value based on params. here rate of change of loss based on our weight/bias.<br/>\n",
    "    * This will inform us how much each weight/bias params influence the loss value if we change each one slightly. <br/>\n",
    "    * We can use this information to change each weight/bias in the direction where it reduce the loss value. <br/>\n",
    "    * But we can't directly use the gradient values to update weight/bias.\n",
    "    \n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda99610",
   "metadata": {},
   "source": [
    "##### Why can't  we use exact gradient to update the params?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95374f",
   "metadata": {},
   "source": [
    "<a href='https://youtu.be/o6FfdP2uYh4' target='_blank'>Cornell CS4780 SP17</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbf464",
   "metadata": {},
   "source": [
    "* Taylor expansion for the derivative.\n",
    "* Function is convex or not.\n",
    "* Taylor expansion holds for small learning rate.\n",
    "* ADAGRAD\n",
    "* Newtons method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc212ff6",
   "metadata": {},
   "source": [
    "##### Taylor expansion for the derivatives of the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6aeb9",
   "metadata": {},
   "source": [
    "<a href='https://www.khanacademy.org/math/ap-calculus-bc/bc-series-new/bc-10-11/v/maclaurin-and-taylor-series-intuition' target='_blank'>Finding Taylor polynomial approximations of functions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c9cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea5312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179202cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_main] *",
   "language": "python",
   "name": "conda-env-python_main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
